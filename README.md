# Рекомендательная система MORE.Tech 4.0

## Фишки
1. Персонализированное обучение - обучение нескольких небольших моделей: KNN, Logistic Classification, RandomForest, Catboost для КАЖДОГО пользователя. Небольшой размер и небольшая вычислительная сложность позволяют дообучать модели в реальном времени и показывать релевантный результат.

2. A/B-тестирование каждой модели. Вполне возможна ситуация, когда пользователи будут предпочитать не лучшую модель по абстрактной метрике, а ту, которая показывает более разнообразные (или наоборот, более точные) новости. *Например, несмотря на объективно более высокое качество звука у цифровых носителей, многие предпочитают "теплый ламповый виниловый" звук*.  

3. Режим Master-Slave. Модели обучаются по тем новостям, которые выбрал опытный, Senior/VIP/Top Master-пользователь. Обычному, Slave пользователю предлагаются новости на основе выбора мастера.

4. С.П.Е.К.Т.Р - мы "разбавляем" и перемешиваем результаты разных моделей, добавляем случайные новости для более полного охвата и более точного тестирования моделей.

4.1 Режим визуализации "СПЕКТРА" - Аура7. 
4.1.1 Подсвечиваем результат в зависимости от уровня уверенности, который дает каждая модель (или мастер). 
4.1.2 -//- в зависимости от совпадения выбора Мастера и персональной рекомендации. *Например, полное совпадение подкрашивает результат в яркий красный цвет. Частичное совпадение с разными категориями - оттенки бежевого, розового и апельсинового.*

Сочетание семи моделей, как в спектре, дает все возможные сочетания цветов. Должно получиться красиво.



## V2 - flask, сервер https://search.steelfeet.ru/ 

для локального запуска надо запустить файл main.py, сайт будет доступен по:
http://192.168.1.6:5000/

## Структура проекта
### Служебные файлы
config.py - параметры подключения к БД. Для безопасности файл занесен в .gitignore и в общем репозитории не отображается. Пример его структуры в файле config_example.py:  

db.py - классы со структурой таблиц БД   

edufunc.py - общие для всего проекта функции  

Crauler_news.py - бот, "паук", краулер - парсит ссылки на внутренние страницы сайтов. Необходим для максимально быстрого появления новостей в результатах поиска. Парсит ТОЛЬКО страницы 1го уровня. Парсит в таблицы News и Search  
https://search.steelfeet.ru/crauler_news  
http://192.168.26.1:5000/crauler_news  

Log - вывод логов работы
https://search.steelfeet.ru/log  
http://192.168.1.6:5000/log  


Parser - парсит текст с найденных ссылок, лемматизирует его и индексирует.   
https://search.steelfeet.ru/parser_news  
http://192.168.1.6:5000/parser_news  


Show_news - вывод релевантных новостей
https://search.steelfeet.ru/news  
http://192.168.1.6:5000/news  


Teach_news - обучение моделей
https://search.steelfeet.ru/teach_news  
http://192.168.1.6:5000/teach_news  

Все эти модули вызываются по крону с необходимой частотой. Модульная структура, помимо прочего, позволяет балансировать нагрузку и управлять скоростью работы ботов. Например, в начале работы, когда БД не заполнена, необходимо запустить несколько экземпляров краулера и парсера для ее первоначального заполнения. В дальнейшем, можно оставить работающими 1-2.

Users - просмотр пользователей  



### Структура БД

Используется база данных SQLite. Но может использоваться и любая другая БД, поддерживаемая SQLAlchemy: PostgresSQL, MySQL и т.д. Описание таблиц в модуле db.py

Таблица News - Таблица с содержимым страниц сайта
    __tablename__ = 'news'
    id = Column(Integer, primary_key=True, autoincrement=True)
    donor = Column(String(255))
    title = Column(String(512))
    href = Column(String(512))
    parse_date = Column(Integer)
    html = Column(Text)
    text = Column(Text)
    lemmas = Column(Text) # набор лемм из текста (мешок слов)
    level = Column(Integer) # уровень вложенности ссылки от корня сайта
    status = Column(Integer) # статус ссылки, 
    
    crauler_news.wsgi  
    0 - найдена, не спарсена  

    parser.wsgi  
    1 - спарсена;
    4 - ошибка при парсинге (используется при отладке)



Таблица Lemmas - Таблица с лемматизированным содержимым страниц
    class Lemmas(Base):
        __tablename__ = 'lemmas'
        id = Column(Integer, primary_key=True, autoincrement=True)
        id_news = Column(Integer)
        lemma = Column(String(255), index=True) # одна лемма из текста

        def __init__(self, id_news, lemma):
            self.id_news = id_news
            self.lemma = lemma

        def __repr__(self):
            return "<Lemma('%s')>" % (self.lemma)




Таблица Log - логирование работы поисковой системы
    __tablename__ = 'log'
    id = Column(Integer, primary_key=True, autoincrement=True)
    action = Column(String(64))
    status = Column(String(64))
    time = Column(Integer)
    donor = Column(String(64))


Эти таблицы не используются, данные для MVP в файле конфигурации config.py  
class Users(Base):
    __tablename__ = 'users'
    id = Column(Integer, primary_key=True, autoincrement=True)
    name = Column(String(512)) 

    def __init__(self, name):
        self.name = name

    def __repr__(self):
        return "<Users('%s')>" % ()


class UsersAction(Base):
    __tablename__ = 'users_action'
    id = Column(Integer, primary_key=True, autoincrement=True)
    user_id = Column(Integer) # идентификатор пользователя
    show_id = Column(Integer) # идентификатор новости, которая была показана
    click_id = Column(Integer) # идентификатор новости, по которой кликнул
    model = Column(String(512)) # какая модель использовалась для поиска релевантных новостей

    def __init__(self, user_id, show_id, click_id, model):
        self.user_id = user_id
        self.show_id = show_id
        self.click_id = click_id
        self.model = model

    def __repr__(self):
        return "<UsersAction('%s','%s','%s')>" % (self.user_id, self.show_id, self.click_id)










## Установка

В зависмости от хостинга, действия могут различаться. Пример установки для сервера Jino приведен в [документации](docs/README.md).   

Для запуска поисковой системы Вам необходимо установить эти модули:  
pip install sqlalchemy  
pip install bs4  
pip install lxml   
pip install tldextract   
pip install nltk   
pip install pymorphy2  
pip install pymorphy2-dicts  
pip install sqlalchemy-utils  
pip install matplotlib  


## Запуск
python main.py  
Сайт будет доступен по ссылкам выше по Вашему локальному адресу, например http://192.168.1.6:5000/  


## Достоинства
- не используются сложные проприетарные поисковые движки: Sphinx, Elasticsearch и т.п. так как помимо закрытого кода они требуют установки на VPS или dedicated сервера. Наша поисковая система легко работает на обычном shared хостинге с поддержкой Python и Flask;
- программа позволяет проиндексировать текстовое содержимое страниц сайта на русском языке по заданному списку адресов сайтов;
- программа позволяет осуществить поиск по текстовому содержимому проиндексированного сайта и выдать наиболее релевантные результаты;
- программа имеет удобный, интуитивно понятный интерфейс для поиска и отображения списка полученных результатов
- поддерживается формат robots.txt от Яндекс;
- система написана в модульном стиле и опубликована на публичном сервере Gogs, что позволяет разработчиком легко дописывать или изменять определённые модули системы под свои нужды;
- система имеет исчерпывающую [документацию и инструкцию](docs/README.md) по развёртыванию системы на своём “железе” сторонними разработчиками.




## История изменений


v2.0039 [main_1.db]  
Новости, заинтересовавшие Краузе

v2.0038 [clear stat]  

v2.0037 [presentation]  

v2.0036.1 [fix]  
v2.0036 [fix]  

v2.0035 [crauler_lenta, local craulers/parsers]  

v2.0034 [trends, inside]  
краткое описание наших задумок для хакатона  

v2.0033.1 [random news backcolor]  
постепенное уменьшение цветов - для демонстрации

v2.0033 [random news backcolor]  

v2.0032 [pagination]  

v2.0031.1 [fix]  

v2.0031 [users on News]  
и расширенное описание ползователей

v2.0030.1 [Show_news_users_db.py]  
Show_news_users_db.py - работа с пользователями из БД  

v2.0030 [Users]  

v2.0029 [wtb_roles, wtb_users]
описание пользователей и ролей в config.py

v2.0028.3 [readme]

v2.0028.2 [readme]

v2.0028.1 [roles => categories]

v2.0028 [select roles]  
описание категорий в config.py (чтобы лишний раз не трогать базу при получении задачи хакатона)

v2.0027 [Teach.py]  
преобразуем в формат для обучения

v2.0026 [olympics]  
перенос в отдельный проект олимпиад и хакатонов

v2.0025 [add_roles.py]  
но не добавлены пока

v2.0024 [Roles]  

v2.0023 [links_4pars]  

v2.0022.2 [fix]  
v2.0022.1 [fix]  
v2.0022 [vedomosti, crauler test]  

v2.0021 [fix]  
db_session.close(), весь вывод через flask.render_template, все в try/except

v2.0020 [go]

v2.0019 [new user]

v2.0018 [UsersAction]

v2.0017 [render_template]  

v2.0016.3 [fix]  

v2.0016.2 [fix]  

v2.0016.1 [fix]  

v2.0016 [news content in DB]  
парсим не только нужные ссылки, но и только нужный контент страниц  

v2.0015 [show news]  
--
некрасивый вывод подписи к ссылке: показывется заголовок, меню, но не сам контент

v2.0014 [admin menu]  

v2.0013 [no html]
не сохраняем html в БД

v2.0012.2 [host in log] 

v2.0012.1 [no host in log] 

v2.0012 [db Users model] 

v2.0011 [Show_news] 

v2.0010 [db Users]  

v2.0009 [parser_news]  

v2.0009 [log]  

v2.0008 [rbc crauler]  
--
vtb подтягивает данные по апи, поэтому ему надо свой формат краулера  

v2.0007.1 [lenta crauler]  

v2.0006 [npedkol crauler]  
отдельный файл парсинга для каждого сайта  

v2.0005 [only news]  
поиск только новостей по классу ссылки  
--  
для каждого сайта нужен свой подход выделения новостей, универсальный не получается  
делать собственный парсер для каждого сайта неудобно: много повторяющегося кода, много cron запросов (хотя это позволило бы задать периодичность парсинга для каждого сайта), много ссылок в главном файле приложения  
=> поэтому делаем отдельные классы для каждого сайта, но вызываем их из единой точки: Crauler_news.py

v2.0004 [Crauler_news]
рабочий Crauler_news

v2.0003 [python modules]
актуальные модули

v2.0002 [new sqlite DB]
сгенерировали новую БД

v2.0001 [phusionpassenger]
достаточно добавить файл passenger_wsgi.py, для запуска приложения на flask на reg.ru


v2 - перенос на flask, sqlite





